{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 5: Build a seq2seq model for machine translation.\n",
    "\n",
    "### Name: Michael DiGregorio\n",
    "\n",
    "### Task: Translate English to Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read and run my code.\n",
    "2. Complete the code in Section 1.1 and Section 4.2.\n",
    "\n",
    "    * Translation **English** to **German** is not acceptable!!! Try another pair of languages.\n",
    "    \n",
    "3. **Make improvements.** Directly modify the code in Section 3. Do at least one of the two. By doing both correctly, you will get up to 1 bonus score to the total.\n",
    "\n",
    "    * Bi-LSTM instead of LSTM.\n",
    "        \n",
    "    * Attention. (You are allowed to use existing code.)\n",
    "    \n",
    "4. Evaluate the translation using the BLEU score. \n",
    "\n",
    "    * Optional. Up to 1 bonus scores to the total.\n",
    "    \n",
    "5. Convert the notebook to .HTML file. \n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "\n",
    "6. Put the .HTML file in your Google Drive, Dropbox, or Github repo.  (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "7. Submit the link to the HTML file to Canvas.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: \n",
    "\n",
    "To implement ```Bi-LSTM```, you will need the following code to build the encoder. Do NOT use Bi-LSTM for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Bidirectional, Concatenate, LSTM\n",
    "\n",
    "# encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
    "#                                   dropout=0.5, name='encoder_lstm'))\n",
    "# _, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "# state_h = Concatenate()([forward_h, backward_h])\n",
    "# state_c = Concatenate()([forward_c, backward_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am going to rewrite a lot of this becuase its horribly confusing to work with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "1. Download data (e.g., \"deu-eng.zip\") from http://www.manythings.org/anki/\n",
    "2. Unzip the .ZIP file.\n",
    "3. Put the .TXT file (e.g., \"deu.txt\") in the directory \"./Data/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import numpy\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from IPython.display import SVG\n",
    "from unicodedata import normalize\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load and clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the following blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., filename = 'Data/deu.txt'\n",
    "filename = 'data/spa.txt'\n",
    "\n",
    "# e.g., n_train = 20000\n",
    "n_train = 120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc entry C\n",
      "\n",
      "pairs entry ['No way!', 'Â¡Mangos!', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2175 (CS) & #3843189 (cueyayotl)']\n",
      "\n",
      "clean pairs entry ['no way' 'mangos' 'ccby france attribution tatoebaorg cs cueyayotl']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "doc = load_doc(filename)\n",
    "# split into Language1-Language2 pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_data(pairs)[0:n_train, :]\n",
    "\n",
    "print(f\"doc entry {doc[100]}\\n\")\n",
    "print(f\"pairs entry {pairs[100]}\\n\")\n",
    "print(f\"clean pairs entry {clean_pairs[100]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fix the roof] => [arregla el tejado]\n",
      "[get the book] => [trae el libro]\n",
      "[get the book] => [consigue el libro]\n",
      "[get the book] => [recoge el libro]\n",
      "[get the book] => [traiga el libro]\n",
      "[get the book] => [recoja el libro]\n",
      "[get upstairs] => [anda para arriba]\n",
      "[ghosts exist] => [los fantasmas existen]\n",
      "[give me half] => [dame la mitad]\n",
      "[give me half] => [deme la mitad]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs[i, 0] + '] => [' + clean_pairs[i, 1] + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts:  (120000,)\n",
      "Length of target_texts: (120000,)\n"
     ]
    }
   ],
   "source": [
    "input_texts = clean_pairs[:, 0]\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n",
    "\n",
    "print('Length of input_texts:  ' + str(input_texts.shape))\n",
    "print('Length of target_texts: ' + str(input_texts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of input  sentences: 52\n",
      "max length of target sentences: 102\n"
     ]
    }
   ],
   "source": [
    "# max encoder seq length is the longest line of the input sentences\n",
    "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
    "# max decoder seq length is the longest line of the translated target sentences\n",
    "max_decoder_seq_length = max(len(line) for line in target_texts)\n",
    "\n",
    "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
    "print('max length of target sentences: %d' % (max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** To this end, you have two lists of sentences: input_texts and target_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text processing\n",
    "\n",
    "### 2.1. Convert texts to sequences\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(Model):\n",
    "    def __init__(self, latent_dim: int, epochs: int):\n",
    "        super(Translator, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    \n",
    "    def call(self, english_sentences):\n",
    "        input_seq, input_token_index = self.generate_input_sequences(english_sentences)\n",
    "        \n",
    "        states_value = self.encoder_model.predict(input_seq)\n",
    "\n",
    "        target_seq = numpy.zeros((1, 1, self.num_decoder_tokens))\n",
    "        target_seq[0, 0, self.target_token_index['\\t']] = 1.\n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_sentence = ''\n",
    "        while not stop_condition:\n",
    "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "            # this line of code is greedy selection\n",
    "            # try to use multinomial sampling instead (with temperature)\n",
    "            sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "            sampled_char = self.reverse_target_char_index[sampled_token_index]\n",
    "            decoded_sentence += sampled_char\n",
    "\n",
    "            if (sampled_char == '\\n' or\n",
    "               len(decoded_sentence) > self.max_decoder_seq_length):\n",
    "                stop_condition = True\n",
    "\n",
    "            target_seq = numpy.zeros((1, 1, self.num_decoder_tokens))\n",
    "            target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "            states_value = [h, c]\n",
    "\n",
    "        return decoded_sentence\n",
    "    \n",
    "    def generate_input_sequences(self, sentences):\n",
    "        seqs = self.encoder_tokenizer.texts_to_sequences(sentences)\n",
    "        encoder_input_seq = pad_sequences(seqs, maxlen=self.max_encoder_seq_length, padding='post')\n",
    "        encoder_input_data = self.onehot_encode(encoder_input_seq, self.max_encoder_seq_length, self.num_encoder_tokens)\n",
    "        return encoder_input_data, self.encoder_tokenizer.word_index\n",
    "\n",
    "    \n",
    "    def fit(self, input_text, target_text):\n",
    "        # generate sequence constraints\n",
    "        # max encoder seq length is the longest line of the input sentences\n",
    "        self.max_encoder_seq_length = max(len(line) for line in input_text)\n",
    "        # max decoder seq length is the longest line of the translated target sentences\n",
    "        self.max_decoder_seq_length = max(len(line) for line in target_text)\n",
    "        \n",
    "        # process the input text\n",
    "        self.encoder_tokenizer, encoder_input_seq, input_token_index = self.text2sequences(self.max_encoder_seq_length, \n",
    "                                                      input_text)\n",
    "        self.decoder_tokenizer, decoder_input_seq, target_token_index = self.text2sequences(self.max_decoder_seq_length, \n",
    "                                                       target_text)\n",
    "        self.target_token_index = target_token_index\n",
    "        print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
    "        print('shape of input_token_index: ' + str(len(input_token_index)))\n",
    "        print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
    "        print('shape of target_token_index: ' + str(len(target_token_index)))\n",
    "        self.num_encoder_tokens = len(input_token_index) + 1\n",
    "        self.num_decoder_tokens = len(target_token_index) + 1\n",
    "\n",
    "        print('num_encoder_tokens: ' + str(self.num_encoder_tokens))\n",
    "        print('num_decoder_tokens: ' + str(self.num_decoder_tokens))\n",
    "        print(target_text[100])\n",
    "        print(decoder_input_seq[100, :])\n",
    "        \n",
    "        encoder_input_data = self.onehot_encode(encoder_input_seq, \n",
    "                                                self.max_encoder_seq_length, \n",
    "                                                self.num_encoder_tokens)\n",
    "        \n",
    "        decoder_input_data = self.onehot_encode(decoder_input_seq, \n",
    "                                                self.max_decoder_seq_length, \n",
    "                                                self.num_decoder_tokens)\n",
    "\n",
    "        decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
    "        decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "        \n",
    "        decoder_target_data = self.onehot_encode(decoder_target_seq, \n",
    "                                        self.max_decoder_seq_length, \n",
    "                                        self.num_decoder_tokens)\n",
    "        \n",
    "        print(encoder_input_data.shape)\n",
    "        print(decoder_input_data.shape)\n",
    "        \n",
    "        # Reverse-lookup token index to decode sequences back to something readable.\n",
    "        self.reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "        self.reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "        \n",
    "        self.encoder_model, self.decoder_model, self.model = self.generate_models()\n",
    "\n",
    "        self.plot_model(self.encoder_model, 'encoder2.pdf')\n",
    "        self.plot_model(self.decoder_model, 'decoder2.pdf')\n",
    "        self.plot_model(self.model, 'model2.pdf')\n",
    "        \n",
    "        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "        self.model.fit([encoder_input_data, decoder_input_data],  # training data\n",
    "                  decoder_target_data,                       # labels (left shift of the target sequences)\n",
    "                  batch_size=64, epochs=self.epochs, validation_split=0.2)\n",
    "    \n",
    "    def generate_models(self):\n",
    "        encoder_inputs = Input(shape=(None, self.num_encoder_tokens),\n",
    "                       name='encoder_inputs')\n",
    "        \n",
    "        encoder_bilstm = Bidirectional(LSTM(self.latent_dim, return_state=True, \n",
    "                                          dropout=0.5, name='encoder_lstm'))\n",
    "        _, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "        state_h = Concatenate()([forward_h, backward_h])\n",
    "        state_c = Concatenate()([forward_c, backward_c])\n",
    "        print(state_h.shape)\n",
    "        print(state_c.shape)\n",
    "        encoder_model = Model(inputs=encoder_inputs, \n",
    "                              outputs=[state_h, state_c],\n",
    "                              name='encoder')\n",
    "        \n",
    "        # inputs of the decoder network\n",
    "        decoder_input_h = Input(shape=(2*self.latent_dim,), name='decoder_input_h')\n",
    "        decoder_input_c = Input(shape=(2*self.latent_dim,), name='decoder_input_c')\n",
    "        decoder_input_x = Input(shape=(None, self.num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "        # set the LSTM layer\n",
    "        decoder_lstm = LSTM(2*self.latent_dim, return_sequences=True, \n",
    "                            return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "        decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
    "                                                              initial_state=[decoder_input_h, decoder_input_c])\n",
    "\n",
    "        # set the dense layer\n",
    "        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "        decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "        # build the decoder network model\n",
    "        decoder_model= Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
    "                             outputs=[decoder_outputs, state_h, state_c],\n",
    "                             name='decoder')\n",
    "\n",
    "        # input layers\n",
    "        encoder_input_x = Input(shape=(None, self.num_encoder_tokens), name='encoder_input_x')\n",
    "        decoder_input_x = Input(shape=(None, self.num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "        # connect encoder to decoder\n",
    "        encoder_final_states = encoder_model([encoder_input_x])\n",
    "        decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
    "        decoder_pred = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "        model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "                      outputs=decoder_pred, \n",
    "                      name='model_training')\n",
    "        \n",
    "        return encoder_model, decoder_model, model\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_model(model, outfile):\n",
    "        SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "        plot_model(\n",
    "            model=model, show_shapes=False,\n",
    "            to_file=outfile\n",
    "        )\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "    @staticmethod\n",
    "    def text2sequences(max_len, lines):\n",
    "        tokenizer = Tokenizer(char_level=True, filters='')\n",
    "        tokenizer.fit_on_texts(lines)\n",
    "        seqs = tokenizer.texts_to_sequences(lines)\n",
    "        seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "        return tokenizer, seqs_pad, tokenizer.word_index\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot_encode(sequences, max_len, vocab_size):\n",
    "        n = len(sequences)\n",
    "        data = numpy.zeros((n, max_len, vocab_size))\n",
    "        for i in range(n):\n",
    "            data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq: (120000, 52)\n",
      "shape of input_token_index: 27\n",
      "shape of decoder_input_seq: (120000, 102)\n",
      "shape of target_token_index: 29\n",
      "num_encoder_tokens: 28\n",
      "num_decoder_tokens: 30\n",
      "\tmangos\n",
      "\n",
      "[13 15  3  6 22  4  5 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n",
      "(120000, 52, 28)\n",
      "(120000, 102, 30)\n",
      "(None, 512)\n",
      "(None, 512)\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None, 28)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional [(None, 512), (None, 583680      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 512)          0           bidirectional_10[0][1]           \n",
      "                                                                 bidirectional_10[0][3]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 512)          0           bidirectional_10[0][2]           \n",
      "                                                                 bidirectional_10[0][4]           \n",
      "==================================================================================================\n",
      "Total params: 583,680\n",
      "Trainable params: 583,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x (InputLayer)    [(None, None, 30)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 512),  1112064     decoder_input_x[0][0]            \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     15390       decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,127,454\n",
      "Trainable params: 1,127,454\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_training\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_x (InputLayer)    [(None, None, 28)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x (InputLayer)    [(None, None, 30)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            [(None, 512), (None, 583680      encoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 512),  1112064     decoder_input_x[0][0]            \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 encoder[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     15390       decoder_lstm[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,711,134\n",
      "Trainable params: 1,711,134\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 50s 32ms/step - loss: 0.7500 - val_loss: 0.7741\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 47s 31ms/step - loss: 0.5399 - val_loss: 0.6851\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 47s 31ms/step - loss: 0.5019 - val_loss: 0.6363\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 47s 31ms/step - loss: 0.4790 - val_loss: 0.6047\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 47s 32ms/step - loss: 0.4622 - val_loss: 0.5855\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.4502 - val_loss: 0.5626\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.4409 - val_loss: 0.5510\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.4335 - val_loss: 0.5436\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.4253 - val_loss: 0.5340\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.4214 - val_loss: 0.5240\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.4150 - val_loss: 0.5186\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 52s 35ms/step - loss: 0.4106 - val_loss: 0.5136\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 52s 35ms/step - loss: 0.4075 - val_loss: 0.5072\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 54s 36ms/step - loss: 0.4030 - val_loss: 0.5011\n",
      "Epoch 15/30\n",
      " 797/1500 [==============>...............] - ETA: 21s - loss: 0.3990"
     ]
    }
   ],
   "source": [
    "translator = Translator(latent_dim=256, epochs=30)\n",
    "translator.fit(input_texts, target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translator([\"I love you\", \n",
    "                  \"how are you\", \n",
    "                  \"I am good at cooking\", \n",
    "                  \"good morning\", \n",
    "                  \"the sky is blue\", \n",
    "                  \"I am very tall\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the translation using BLEU score\n",
    "\n",
    "Reference: \n",
    "- https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "- https://en.wikipedia.org/wiki/BLEU\n",
    "\n",
    "\n",
    "**Hint:** \n",
    "\n",
    "- Randomly partition the dataset to training, validation, and test. \n",
    "\n",
    "- Evaluate the BLEU score using the test set. Report the average.\n",
    "\n",
    "- A reasonable BLEU score should be 0.1 ~ 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs583",
   "language": "python",
   "name": "cs583"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
